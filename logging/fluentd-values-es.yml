---

# Fluentd image
image:
  repository: "ricsanfre/fluentd-aggregator"
  pullPolicy: "IfNotPresent"
  tag: "v1.15.2-debian-1.0"

# Deploy fluentd as deployment
kind: "Deployment"
# Number of replicas
replicaCount: 1
# Enabling HPA
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

# Do not create serviceAccount and RBAC. Fluentd does not need to get access to kubernetes API.
serviceAccount:
  create: false
rbac:
  create: false

## Additional environment variables to set for fluentd pods
env:
  # Path to fluentd conf file
  - name: "FLUENTD_CONF"
    value: "../../../etc/fluent/fluent.conf"
    # Elastic operator creates elastic service name with format cluster_name-es-http
  - name:  FLUENT_ELASTICSEARCH_HOST
    value: efk-es-http
    # Default elasticsearch default port
  - name:  FLUENT_ELASTICSEARCH_PORT
    value: "9200"
  # Elasticsearch user
  - name: FLUENT_ELASTICSEARCH_USER
    value: "elastic"
  # Elastic operator stores elastic user password in a secret
  - name: FLUENT_ELASTICSEARCH_PASSWORD
    valueFrom:
      secretKeyRef:
        name: "efk-es-elastic-user"
        key: elastic
  # Setting a index-prefix for fluentd. By default index is logstash
  - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
    value: fluentd
  - name: FLUENT_ELASTICSEARCH_LOG_ES_400_REASON
    value: "true"
 
# Volumes and VolumeMounts (only configuration files and certificates)
volumes:
  - name: etcfluentd-main
    configMap:
      name: fluentd-main
      defaultMode: 0777
  - name: etcfluentd-config
    configMap:
      name: fluentd-config
      defaultMode: 0777

volumeMounts:
  - name: etcfluentd-main
    mountPath: /etc/fluent
  - name: etcfluentd-config
    mountPath: /etc/fluent/config.d/

# Service. Exporting forwarder port (Metric already exposed by chart)
service:
  type: "ClusterIP"
  annotations: {}
  ports:
  - name: forwarder
    protocol: TCP
    containerPort: 24224

## Fluentd list of plugins to install
##
plugins: []
# - fluent-plugin-out-http

## Do not create additional config maps
##
configMapConfigs: []

## Fluentd configurations:
##
fileConfigs:
  01_sources.conf: |-
    ## logs from fluentbit forwarders
    <source>
      @type forward
      @label @FORWARD
      bind "#{ENV['FLUENTD_FORWARD_BIND'] || '0.0.0.0'}"
      port "#{ENV['FLUENTD_FORWARD_PORT'] || '24224'}"
    </source>
    ## Enable Prometheus end point
    <source>
      @type prometheus
      @id in_prometheus
      bind "0.0.0.0"
      port 24231
      metrics_path "/metrics"
    </source>
    <source>
      @type prometheus_monitor
      @id in_prometheus_monitor
    </source>
    <source>
      @type prometheus_output_monitor
      @id in_prometheus_output_monitor
    </source>
  02_filters.conf: |-
    <label @FORWARD>
      # Re-route fluentd logs. Discard them
      <match kube.var.log.containers.fluentd**>
        @type relabel
        @label @FLUENT_LOG
      </match>
      ## Get kubernetes fields
      <filter kube.**>
        @type record_modifier
        remove_keys kubernetes, __dummy__, __dummy2__
        <record>
          __dummy__   ${ p = record["kubernetes"]["labels"]["app"]; p.nil? ? p : record['app'] = p; }
          __dummy2__   ${ p = record["kubernetes"]["labels"]["app.kubernetes.io/name"]; p.nil? ? p : record['app'] = p; }
          namespace ${ record.dig("kubernetes","namespace_name") }
          pod ${ record.dig("kubernetes", "pod_name") }
          container ${ record.dig("kubernetes", "container_name") }
          node_name ${ record.dig("kubernetes", "host")}
        </record>
      </filter>
      ## Avoid ES rejection due to conflicting field types when using fluentbit merge_log
      <filter kube.**>
        @type record_transformer
        enable_ruby true
        remove_keys log_processed
        <record>
          json_message.${record["container"]} ${(record.has_key?('log_processed'))? record['log_processed'] : nil}
        </record>
      </filter>
      <match **>
        @type relabel
        @label @DISPATCH
      </match>
    </label>
  03_dispatch.conf: |-
    <label @DISPATCH>
      # Calculate prometheus metrics
      <filter **>
        @type prometheus
        <metric>
          name fluentd_input_status_num_records_total
          type counter
          desc The total number of incoming records
          <labels>
            tag ${tag}
            hostname ${hostname}
          </labels>
        </metric>
      </filter>
      # Copy log stream to different outputs
      <match **>
        @type copy
        <store>
          @type relabel
          @label @OUTPUT_ES
        </store>
        <store>
          @type relabel
          @label @OUTPUT_STDOUT
        </store>  
      </match>
    </label>
  04_outputs.conf: |-
    <label @OUTPUT_ES>
      ## Avoid ES rejection due to conflicting field types when using fluentbit merge_log
      <filter kube.**>
        @type record_transformer
        enable_ruby true
        remove_keys log_processed
        <record>
          message_${record["container"]} ${(record.has_key?('log_processed'))? record['log_processed'] : nil}
        </record>
      </filter>
      # Send received logs to elasticsearch
      <match **>
        @type elasticsearch_data_stream
        @id out_es
        @log_level info
        data_stream_name fluentd
        include_tag_key true
        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        scheme http
        user "#{ENV['FLUENT_ELASTICSEARCH_USER'] || use_default}"
        password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD'] || use_default}"
        reload_connections false
        reconnect_on_error true
        reload_on_failure true
        request_timeout 15s
        time_key time
        include_timestamp true
        suppress_type_name true
        sniffer_class_name Fluent::Plugin::ElasticsearchSimpleSniffer
        enable_ilm true
        <buffer>
          flush_thread_count 8
          flush_interval 5s
          chunk_limit_size 2M
          queue_limit_length 32
          retry_max_interval 30
          retry_forever true
        </buffer>
      </match>
    </label>
    <label @OUTPUT_STDOUT>
      # Send received logs to elasticsearch
      <match **>
        @type stdout
      </match>
    </label>

